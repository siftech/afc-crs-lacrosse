# Patch Testing

This directory is the home for testing patches in a tighter loop than prt, avoiding the full lacrosse startup.

---

## Table of Contents

- [Registering Tests](#registration)
- [Organization](#organization)
- [Running Tests](#running)
- [Status](#status)

---

## Registration

Registering a test problem means adding a subdirectory for it to the `problems` subdirectory of this directory and adding the expected assets to it.
They aren't there by default in the interest of saving space/clutter.
The assets expected to be added to a problem directory are domain-specific, where at this time there are two domains: `nginx cpvs` and `juliet`.
These assets are all the grist needed to ask the LLM for a patch, and assess the result.

There are two separate scripts for registering problems in the two domains:
### Juliet Domain:
Currently this script will only run correctly on the `lacrosse-0` server, because it looks for raw problem data in Rick's home directory there.

```
register-juliet-patch-test-problems
```

By default this currently registers all the problems for the CWE416 which Rick/Dave's scripting has deemed suitable. You may want to customize (filter or expand) the set of problems that get registered.  To change this, edit the script directly at `code/tools/register-juliet-patch-test-problems`.  You'll need to add entries or modify values of the `GLOB_GROUPS` associative array.  Adding entries (by adding lines like the one below) is how we can expand to new CWE's, once ready. To filter to a different set within each cateogry, we modify the glob value (`CWE416_Use_After_Free*`). This glob value is a space-separated list (currently length 1), so we can add multiple subsets of a category.
```
# Define the nested arrays (lower-level globs) for each higher-level category
# Each list of globs (value) is space-separated.
GLOB_GROUPS["juliet_cwe_416/test_setup/cwe416_2025feb19"]="CWE416_Use_After_Free*"
```

### NGINX CPVs Domain

You don't need to run (or care about) this, because we only have 6 nginx cpv's available so far and all of them have been registered and committed already. But just for reference, the relevant scripts are:
```
extract-lax-ref # Extract an initial cpv directory from Mike's lax-ref stuff in his home dir on lacrosse-0
setup-lax-ref-patch-data # Do additional prep work to get the cpv pieces in place
register-cpv-patch-test-problem  # to be run from a curated cpv directory
```

## Organization

Each subdir in the problems subdir is a patch-testing "problem".
One juliet problem is committed for reference, to verify the registration script above is working on other juliet problems:

### The Sample Juliet Problem:
[juliet_CWE416_Use_After_Free__return_freed_ptr_01](problems/juliet_CWE416_Use_After_Free__return_freed_ptr_01):
```
bad_err*  bad_source/  good_err*  good_source/
```
`bad_err` is the sanitizer stderr output of the bad code.
`bad_source` simulates a repo containing the bad code.  Currently just contains one file.

`good_err` and `good_source` are the corresponding "good" components.  **Note**: We currently don't use these.  We had notions that perhaps
we could use them to generate additional tests for the patched code (e.g. expecting the patched stdout/stderr to match the "good" stdout/stderr)
but initial investigation suggests it's not that simple.

### NGINX CPV Problems:

The problem contents here are more complicated, since we have the bic and associated stderr logs.  We also have a blob and a harness.  All of these get used, depending on the pipeline used.
FIXME: Explain further.

### Results

The [results](results) subdirectory will be populated with a subdir for each tested problem after running tests.
Each problem's results subdir will contain subdirs for each patching "approach" that has run (always contains the most recent results).
A pipeline "approach" is the name of the pipeline with the name of the LLM used appended to the end.

- patch-test
  - results
    - juliet_CWE416_Use_After_Free__return_freed_ptr_01
      - full_scan_rewrite_given_sanitizer_gpt4o
        - patch
        - clean_patch
        - make-patch
        - test-patch

The `patch` is the one generated by the pipeline.
`clean_patch` (only applies to juliet) is the version that gets tested. It has the git preamble removed/replaced, since this prevents it from applying outside a git repo, at least the way we currently apply.
`make-patch` is a dir containing more logs and artifacts from the patch generation process.
`test-patch` is a dir containing more logs and artifacts from the patch testing process.

## Running

Inspired by the brevity of `prt`, patch tests are run by the command `pt`!  There is also a parallel version available `pt-parallel` but **prt-parallel is only meant to work on juliet problems** because the nginx cpv problems use docker to test.

**These must be run from the patch-test directory.**

This command tests a set of juliet problems in parallel.  Note the first arg is a pipeline name (see `code/dspy/pipelines`), the second is a default LLM to use in the pipeline ("default" because for some minor steps it makes sense to always use a fixed LLM), and the third is a glob specifying a set of tests.
```
$ pt-parallel full_scan_rewrite_given_sanitizer gpt4o problems/juliet_CWE416_Use_After_Free*
passing apply list: results/prev-batch-pass-fail/pass-apply.log
passing build list: results/prev-batch-pass-fail/pass-build.log
passing run_pov list: results/prev-batch-pass-fail/pass-run_pov.log
passing run_tests list: results/prev-batch-pass-fail/pass-run_tests.log
failing apply list: results/prev-batch-pass-fail/fail-apply.log
failing build list: results/prev-batch-pass-fail/fail-build.log
failing run_pov list: results/prev-batch-pass-fail/fail-run_pov.log
failing run_tests list: results/prev-batch-pass-fail/fail-run_tests.log

Num passing apply: 118 of 118 (100%)
Num passing build: 118 of 118 (100%)
Num passing run_pov: 114 of 118 (96%)
Num passing run_tests: 0 of 118 (0%)
```
It gives back locations of lists of passing and failing problems.

See the [Results](#results) section above for more pointers to relevant results.

This command tests a set of nginx cpv problems in series:
```
$ pt full_scan_rewrite_given_sanitizer gpt4o problems/cpv*

passing apply list: results/prev-batch-pass-fail/pass-apply.log
passing build list: results/prev-batch-pass-fail/pass-build.log
passing run_pov list: results/prev-batch-pass-fail/pass-run_pov.log
passing run_tests list: results/prev-batch-pass-fail/pass-run_tests.log
failing apply list: results/prev-batch-pass-fail/fail-apply.log
failing build list: results/prev-batch-pass-fail/fail-build.log
failing run_pov list: results/prev-batch-pass-fail/fail-run_pov.log
failing run_tests list: results/prev-batch-pass-fail/fail-run_tests.log

Num passing apply: 6 of 6 (100%)
Num passing build: 5 of 6 (83%)
Num passing run_pov: 2 of 6 (33%)
Num passing run_tests: 2 of 6 (33%)

```
FIXME: Ran out of time to go into more detail (if needed), or run all the pipelines and make sure they work and generate the expected output.  Barely got this working again in time (I think...) after the reorg.

## Status (TODOs, Gotchas, and State of Play)

- Note that the run_pov test for juliet currently just sees whether the sanitizer crashes on the patched code.  This is not a perfect test.  For one thing, it generates some false negatives (at a glance, it seems to account for all of ~3% failures on the whole set of CWE 416).  This is because some non use-after-free memory trip crashes (e.g. memory leaks).  Rick sees this on the good code as well, but (curiously) in much higher numbers.  May be non-determinism or may be a notable difference between good vs. patched.  For a better run_pov test, we might want to look for the same type of sanitizer error as the `bad_err`.  Anything else (no error or different error type) would be a `*PASS*.
- We investigated the possibility of adding additional tests based on the atdout and stderr of the good code, but it seems to be more hairy than we hoped, so we dropped it quick.  For one thing, the inconsistency of other sanitizer crashes between good and patched is a problem.
- We haven't yet circled back to hook up the bar-chart-diplay-server thingy for the nginx cpvs. It currently points to the old expected results location.
- Speaking of the old expected results location, somewhere we are probably creating a rogue `patch_results` directory (or somesuch).  We should hunt this down in the scripting and point it toward a reasonable location, then point the bar-chart display thing at it.
- Note that the .gitignore in this directory is doing a lot.  It's essentially a whitelist instead of a blacklist.  If you try to commit things in this directory and don't see them in `git status`, you'll need to add to the .gitignore. We may want to convert this to a blacklist.

Ran out of time to think of more of these, but there are probably more. More may come to mind this weekend.  All relevant scripts should be committed though.
